{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 1 in Assignment 1\n",
    "#### Name: Peiyu Liu\n",
    "#### SID: 31153291\n",
    "\n",
    "###### Date: 15/04/2021\n",
    "\n",
    "#### Libraries used:\n",
    "* re (Use regular expression matching specific data)  \n",
    "* os (From path to read and write files)\n",
    "* langid (langid.classify() check english data)\n",
    "\n",
    "# 1. Introduction\n",
    "For assignment 1 and task 1, I am provided one package files. I need to understand data's components. Each file has four main parts: uuid, author, published time, text. Read all txt files, gather each txt file together, divide uuid, author, published, and text individually. Gather each part of data into one dictionary. their keys and values need to be matched. Use the langid package to check and delete all data that do not belong to English. Use regular expression to compile special characters and handle Unicode. Write the result to one CSV file and one XML file according to sample format.\n",
    "\n",
    "<u> Details in coding sections </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Assignment coding\n",
    "## import Libraries- os,re,langid\n",
    "* download package: pip install package_name\n",
    "* import package: import package_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import langid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files from directory\n",
    "##### Use os.listdir(path_name)\n",
    "##### endswith() function to limit filename.txt\n",
    "##### for loop to read all txt files in directory\n",
    "##### file.read() function to store all text into one doc\n",
    "* Iterator read all txt files in path directory\n",
    "* Store all content of files in string doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_collections = \" \"\n",
    "path = './31153291'\n",
    "for file_each in os.listdir(path):\n",
    "    if file_each.endswith('txt'):\n",
    "        file_read = open(path + '/' + file_each, 'r', encoding='utf-8')\n",
    "        txt_collections += file_read.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process raw data\n",
    "### Matching data in files to regular expression\n",
    "* Compile regular expression: re.compile() function to compile expression\n",
    "* replace unwanted data: re.sub to replace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_reg = re.compile(r'\\\\u\\S*')\n",
    "clear_collection = re.sub(unicode_reg, '', txt_collections)\n",
    "unicode_regx = re.compile(r'\\?\\?')\n",
    "clear_collection = re.sub(unicode_regx, '', clear_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data to identify uuid, author, published time, text\n",
    "##### According to analysis data, get each part regular expression format\n",
    "##### Use re.findall() function to find all matching data from files' contents\n",
    "* uuid: 63d2b01f75221bbc4517c594789a4e2ab3001e74\n",
    "* author: WILL GRAVES, AP Sports Writer\n",
    "* published: 2021-01-20T10:03:00.000+02:00\n",
    "* text: A Trump fan's best-case scenario for the Biden era Trump children emotional...\n",
    "\n",
    "<u>Notice: after find all data. Use len(id_collections) and len(other) to check length is matching each other.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_collections = re.findall('uuid\": \"(.*?)\"', clear_collection)\n",
    "\n",
    "author_collections = re.findall('author\": \"(.*?)\"', clear_collection)\n",
    "\n",
    "text_collections = re.findall('text\":\\s\"(.*?)\", \"', clear_collection)\n",
    "\n",
    "time_collections = re.findall('published\": \"(.*?)\"', clear_collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents dictionary\n",
    "### Use dicitionary to combine all information, key to value.\n",
    "* Dictionary: {key:value}\n",
    "* Combine each personal information as a group.\n",
    "* Format: uuid: value, author: value, text: value, published: value.\n",
    "* append() function add dictionary to list store space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_combine = []\n",
    "id_check = []\n",
    "for flag1 in range(len(id_collections)):\n",
    "    if id_collections[flag1] not in id_check:\n",
    "        id_check.append(id_collections[flag1])\n",
    "        info_dic = {'uuid': id_collections[flag1],\n",
    "                    'author': author_collections[flag1],\n",
    "                    'text': text_collections[flag1],\n",
    "                    'published': time_collections[flag1]\n",
    "                    }\n",
    "        info_combine.append(info_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check English contents and replace symbol to computer language.\n",
    "### Use langid.classify() function to check English contents and store them.\n",
    "\n",
    "<u>Notice: langid.classify() will reture a tuple, the first place is language type, so I need to write index 0 to check the language.</u>\n",
    "\n",
    "* Throw all non-English contents away.\n",
    "* Read contents from text part to process symbols.\n",
    "* Replace specific symbols or code to language processing format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_check = []\n",
    "for flag2 in info_combine:\n",
    "    content_text = flag2['text']\n",
    "    content_text = re.sub('\\\\\\\\n', '\\n', content_text)\n",
    "    content_text = re.sub('&', '&amp;', content_text)\n",
    "    content_text = re.sub('\\\\\\\\\"', '&quot;', content_text)\n",
    "    content_text = re.sub('<', '&lt;', content_text)\n",
    "    content_text = re.sub('>', '&gt;', content_text)\n",
    "    flag2['text'] = content_text\n",
    "    if langid.classify(flag2['text'])[0] == 'en':\n",
    "        english_check.append(flag2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV output processing\n",
    "### file.write() to output file\n",
    "* Output name should be 31153291.csv, format should be according to sample.csv\n",
    "* Write excel title first: uuid, author, published, text\n",
    "* Use ',' to divide each part \n",
    "\n",
    "I learn how to output csv in tidy way from below website:\n",
    "resource： https://docs.python.org/3/library/csv.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output = open('31153291.csv', 'w')\n",
    "csv_output.write('uuid, author, published, text\\n')\n",
    "for flag3 in english_check:\n",
    "    part_content = flag3['text']\n",
    "    part_content = re.sub('\"', '\"\"', part_content)\n",
    "    csv_output.write(flag3['uuid'] + ',\"' + flag3['author'] + '\",' + flag3['published'] + ',\"' + part_content + '\"' + '\\n')\n",
    "csv_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML output processing\n",
    "### file.write() to output file\n",
    "* If author is empty, change author /author to author/\n",
    "* According to sample.xml\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_output = open('31153291.xml', 'w', encoding='utf-8')\n",
    "xml_output.write('<?xml version=\"1.0\" ?>' + '\\n')\n",
    "xml_output.write('<sample>' + '\\n')\n",
    "\n",
    "for flag4 in english_check:\n",
    "    xml_output.write('\t<item>' + '\\n')\n",
    "    xml_output.write('\t\t<uuid>' + flag4['uuid'] + '</uuid>' + '\\n')\n",
    "    if flag4['author'] == '':\n",
    "        xml_output.write('\t\t<author/>' + '\\n')\n",
    "    else:\n",
    "        xml_output.write('\t\t<author>' + flag4['author'] + '</author>' + '\\n')\n",
    "    xml_output.write('\t\t<published>' + flag4['published'] + '</published>' + '\\n')\n",
    "    xml_output.write('\t\t<text>' + flag4['text'] + '</text>' + '\\n')\n",
    "    xml_output.write('\t</item>' + '\\n')\n",
    "xml_output.write('</sample>' + '\\n')\n",
    "xml_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Summary\n",
    "Task 1 is not a difficult assignment, but it will practice your analysis ability and your capacity to handle data. It is an excellent practice to learn how to observe the data format, organize and classify them through the structure of the data. The efficiency of using python to process data is very convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reference\n",
    "\n",
    "CSV output: https://docs.python.org/3/library/csv.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
