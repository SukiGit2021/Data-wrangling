{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 2 in Assignment 1\n",
    "\n",
    "#### Name: Peiyu Liu\n",
    "#### SID: 31153291\n",
    "\n",
    "###### Date: 15/04/2021\n",
    "\n",
    "#### Libraries used:\n",
    "* re: process regular expression\n",
    "* nltk(FreqDist): calculate words appearance\n",
    "* nltk(PorterStemmer): extract stem of word\n",
    "* nltk(ngrams): generate N-words\n",
    "* nltk(sklearn.CountVectorizer): process word segmentation and matrix\n",
    "* pandas: read file and reshape contents\n",
    "* langid: fitler language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Task 2 provides a tsv document that document contains 1000+ articles. I use the pandas package to read the tsv document and upload all contents in this document. I use pd. concat function to reshape this table and set a new index. I use langid to filter non-English contents. I need to use packages like the \"re\" package to compile the sample regular expression \"\\w+(?:[-']\\w+)?\" and find words that match this regular expression. I use the Porter stemming algorithm in the natural language processing method to extract the stem of words. Then use the FreqDist method to count the number of occurrences of words. To output bigram words, I use the N-grams package to pair word and word. I use the CountVectorizer library of sklearn in the natural language processing method for word segmentation and encode the words in the text and convert them into a sparse matrix.\n",
    "\n",
    "<u>Details will introduce in each sections.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Assignment Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "* download package: pip install package_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import langid\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "from nltk import PorterStemmer\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read tsv file\n",
    "* use pandas to read table format\n",
    "* use pd.concat() function to reshape tables as one table\n",
    "* ignore_index should be true to ignore previous indexs in each tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_upload = pd.read_table('~/Desktop/5196/assignment 1/task2/31153291.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_redo = pd.concat([tsv_upload, tsv_upload], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing contents\n",
    "* use pd.iterrows() function to iterate dataframe, read each line.\n",
    "* return value is [index, line_each]\n",
    "* use langid to check all line, if line is English then store line to dic.\n",
    "\n",
    "\n",
    "reference: iterrows() https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_dic = {}\n",
    "for index, line_each in shape_redo.iterrows():\n",
    "    num_each = line_each['Unnamed: 0']\n",
    "    if langid.classify(line_each.text)[0] != 'en':\n",
    "        continue\n",
    "    if num_each not in shape_dic.keys():\n",
    "        shape_dic[num_each] = [line_each.text]\n",
    "    elif line_each.text not in shape_dic[num_each]:\n",
    "        shape_dic[num_each] += [line_each.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read stop words \n",
    "* read stop words document\n",
    "* split each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_upload = open('stopwords_en.txt', 'r', encoding='utf-8')\n",
    "stop_words = words_upload.read()\n",
    "stop_words = stop_words.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use re to compile sample regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_regx = re.compile(r\"\\w+(?:[-']\\w+)?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter words using stop words and sample regular expression\n",
    "* iterator reads key and value in dictionary\n",
    "* iterator reads each word in each value\n",
    "* use re.findall() to match each word in lower format with sample regular expression\n",
    "* store each capable word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_value = {}\n",
    "token_collection = []\n",
    "for flag in shape_dic.keys():\n",
    "    text_values = shape_dic[flag]\n",
    "    initial_container = []\n",
    "    for word in text_values:\n",
    "        regx_token = re.findall(sample_regx, word)\n",
    "        regx_token = [word.lower() for word in regx_token if word.lower() not in stop_words and len(word) > 2]\n",
    "        initial_container += regx_token\n",
    "        regx_token = list(set(regx_token))\n",
    "        token_collection += regx_token\n",
    "    num_value[flag] = initial_container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule: word appearance 5%-95%\n",
    "* expression to set maximum rate and minimum rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "appearance_word = 0\n",
    "for flag in shape_dic.keys():\n",
    "    appearance_word += len(shape_dic[flag])\n",
    "min_rate = 0.05 * appearance_word\n",
    "max_rate = 0.95 * appearance_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing\n",
    "use PorterStemmer() function to remove the prefix and suffix of the word to get the root\n",
    "* happy=>happiness, where happy is called the stem of happiness.\n",
    "* Reference: PorterStemmer() trainning: https://pythonprogramming.net/stemming-nltk-tutorial/\n",
    "\n",
    "use FreqDist to check word appearance\n",
    "* FreqDist inherits from dict. The key in FreqDist is a word, and the value is the total number of occurrences of the word.\n",
    "* Reference: FreqDist with python https://stackoverflow.com/questions/4634787/freqdist-with-nltk\n",
    "\n",
    "* use maximum rate and minimum rate to check words' appearance to remove unavailable words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_import = PorterStemmer()\n",
    "token_freq = FreqDist(token_collection)\n",
    "collection_new = []\n",
    "for flag in token_freq.keys():\n",
    "    if token_freq[flag] > min_rate and token_freq[flag] < max_rate:\n",
    "        collection_new.append(stemmer_import.stem(flag))\n",
    "collection_new = list(set(collection_new))\n",
    "collection_new = [word for word in collection_new if len(word) > 2 and word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process unigram word rating\n",
    "\n",
    "* sorted all processed articles\n",
    "* Each form of the word counts as the word has appeared, so use stem to extract the stem.\n",
    "* after use stem() to extract words, then use FreqDist() to calculate times.\n",
    "* use most_common() function to rate top 100 appearance words.\n",
    "* Reference: most_common():https://www.geeksforgeeks.org/python-find-most-frequent-element-in-a-list/\n",
    "* process word and time format like sample document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_words = ''\n",
    "for flag in sorted(num_value):\n",
    "    word_token = num_value[flag]\n",
    "    word_token = [stemmer_import.stem(word) for word in word_token]\n",
    "\n",
    "    word_feq = FreqDist(word_token)\n",
    "    feq_record = word_feq.most_common(100)\n",
    "    single_words += str(flag) + ':' + str(feq_record) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output top 100 unigram words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('31153291_100uni.txt', 'w') as output:\n",
    "    output.write(single_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process 200 meaningful bigrams (PMI measure)\n",
    "\n",
    "* PMI: In text processing, it is used to calculate the degree of association between two words.\n",
    "* Reference: PMI: https://medium.com/dataseries/understanding-pointwise-mutual-information-in-nlp-e4ef75ecb57a\n",
    "* use nltk.collocations.BigramAssocMeasures() to get bigram words.\n",
    "* use nltk.collocations.BigramCollocationFinder.from_words to find bigram words,extract the 2grams of any two words in the window, and realize the non-continuous frequent co-occurring word pairs.\n",
    "* Reference: collocations:http://www.nltk.org/howto/collocations.html\n",
    "* For binary phrases, the NBEST instance method based on PMI can be used.\n",
    "* use collocation.nbest() to findg word pairs' frequency is top 200.\n",
    "* Reference: collocation.nbest():https://stackoverflow.com/questions/21128689/how-to-get-pmi-scores-for-trigrams-with-nltk-collocations-python\n",
    "* reshape word pair like a_b and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_bigram = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_container = []\n",
    "for flag in shape_dic.keys():\n",
    "    text_value = shape_dic[flag]\n",
    "    for word in text_value:\n",
    "        initial_container += re.findall(sample_regx, word)\n",
    "initial_container = [word.lower() for word in initial_container if word.lower() not in stop_words and len(word) > 2]\n",
    "collocation = nltk.collocations.BigramCollocationFinder.from_words(initial_container)\n",
    "bigram_collection = collocation.nbest(vocab_bigram.pmi, 200)\n",
    "bigram_collection = [str(word[0] + '_' + word[1]) for word in bigram_collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_collection = collection_new + bigram_collection\n",
    "vocab_collection.sort()\n",
    "vocab_reshape = dict(zip(vocab_collection, [flag for flag in range(len(vocab_collection))]))\n",
    "vocab_container = ''\n",
    "for flag, values in vocab_reshape.items():\n",
    "    vocab_container += flag + \":\" + str(vocab_reshape[flag]) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output vocab document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('31153291_vocab.txt', 'w') as output:\n",
    "    output.write(vocab_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process bigram words\n",
    "\n",
    "* use re.findall() to find words meet sample regular expression.\n",
    "* compare words with lower format.\n",
    "* use ngram to reshape word to word pairs(a,b). A sequence of 2 consecutive words in contents.\n",
    "* Reference: Ngram: https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams\n",
    "* use FreqDist to check word appearance\n",
    "* FreqDist inherits from dict. The key in FreqDist is a word, and the value is the total number of occurrences of the word.\n",
    "* Reference: FreqDist with python https://stackoverflow.com/questions/4634787/freqdist-with-nltk\n",
    "* use most_common() function to extract top 100 word pairs.\n",
    "* Reference: most_common(): https://docs.python.org/3/library/collections.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_container = ''\n",
    "initial_container = []\n",
    "for flag in shape_dic.keys():\n",
    "    text_value = shape_dic[flag]\n",
    "    for word in text_value:\n",
    "        initial_container += re.findall(sample_regx, word)\n",
    "    initial_container = [word.lower() for word in initial_container]\n",
    "    pair_token = list(ngrams(initial_container, 2))\n",
    "    pair_token = FreqDist(pair_token)\n",
    "    pair_token = pair_token.most_common(100)\n",
    "    bigram_container += str(flag) + \":\" + str(pair_token) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output top 100 bigram words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('31135291_100bi.txt', 'w') as output:\n",
    "    output.write(bigram_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process sparse representation\n",
    "* CountVectorizer: It is a text feature extraction method. Only considers the frequency of each vocabulary in the text. CountVectorizer converts the text's words into a word frequency matrix, calculate the number of times each word appears.A model for text vectorization. This model does not consider the grammar and the order of words, but only considers the frequency of occurrence of all words.\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "* The CountVectorizer（） class converts the words in the text into a word frequency matrix.\n",
    "\n",
    "* Calculate the number of occurrences of each word through the fit_transform（） function.\n",
    "\n",
    "* The keywords of all texts in the bag of words can be obtained through get_feature_names().\n",
    "\n",
    "* See the result of the word frequency matrix through toarray().\n",
    "\n",
    "* use dictionary zip()function to gather words and matrix.\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "https://kavita-ganesan.com/how-to-use-countvectorizer/#.YHgEbkj7TkI\n",
    "\n",
    "* check word length is no less than 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_words, token_pattern=\"\\w+(?:[-']\\w+)?\", ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = ''\n",
    "for flag in shape_dic.keys():\n",
    "    reshape_dic = ' '.join(shape_dic[flag])\n",
    "    try:\n",
    "        matrix_data = vectorizer.fit_transform([reshape_dic])\n",
    "    except ValueError:\n",
    "        continue\n",
    "    vocab_match = vectorizer.get_feature_names()\n",
    "    matrix_data = matrix_data.toarray()[0]\n",
    "    vocab_matdic = dict(zip(vocab_match, matrix_data))\n",
    "    matrix_dic = {}\n",
    "    for word, count in vocab_matdic.items():\n",
    "        if count > 0:\n",
    "            ind_word = word.split()\n",
    "            if len(ind_word) == 2:\n",
    "                word_shape = '_'.join(ind_word)\n",
    "            else:\n",
    "                word_shape = stemmer_import.stem(word)\n",
    "\n",
    "            if word_shape in vocab_reshape.keys():\n",
    "                word_indx = vocab_reshape[word_shape]\n",
    "                if word_indx in matrix_dic.keys():\n",
    "                    matrix_dic[word_indx] += vocab_matdic[word]\n",
    "                else:\n",
    "                    matrix_dic[word_indx] = vocab_matdic[word]\n",
    "    regx_token = []\n",
    "    for flag, value in matrix_dic.items():\n",
    "        regx_token.append(str(flag) + \":\" + str(value))\n",
    "    matrix += str(flag) + ',' + ','.join(regx_token) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output count vectorizition document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('31153291_countVec.txt', 'w', encoding='utf-8') as output:\n",
    "    output.write(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Summary\n",
    "\n",
    "Task 2 is a complex question for me. Because I have not master python very well at this stage.  I have to read and find many materials to solve the facing problems. But luckily, I finished task 2, and I learned a lot of methods to handle a massive amount of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "https://kavita-ganesan.com/how-to-use-countvectorizer/#.YHgEbkj7TkI\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams\n",
    "\n",
    "https://stackoverflow.com/questions/4634787/freqdist-with-nltk\n",
    "\n",
    "https://docs.python.org/3/library/collections.html\n",
    "\n",
    "https://medium.com/dataseries/understanding-pointwise-mutual-information-in-nlp-e4ef75ecb57a\n",
    "\n",
    "http://www.nltk.org/howto/collocations.html\n",
    "\n",
    "https://stackoverflow.com/questions/21128689/how-to-get-pmi-scores-for-trigrams-with-nltk-collocations-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
